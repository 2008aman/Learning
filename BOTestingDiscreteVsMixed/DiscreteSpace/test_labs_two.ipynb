{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dca8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float64   # use double precision\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.test_functions.synthetic import Labs\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.constraints import GreaterThan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d2fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABS benchmark with 30 binary variables\n",
    "dim = 30\n",
    "labs = Labs(dim=dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e69733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial best merit factor: 1.3595166163141994\n"
     ]
    }
   ],
   "source": [
    "# Generate initial random binary training data\n",
    "def generate_initial_data(n=8):\n",
    "    X = torch.randint(0, 2, (n, dim), dtype=dtype, device=device)\n",
    "    Y = labs(X).unsqueeze(-1)  # LABS merit factor values\n",
    "    return X, Y\n",
    "\n",
    "train_X, train_Y = generate_initial_data()\n",
    "print(\"Initial best merit factor:\", train_Y.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7343a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleTaskGP(train_X=train_X, train_Y=train_Y)\n",
    "model.likelihood.noise_covar.register_constraint(\"raw_noise\", GreaterThan(1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed9ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = ExactMarginalLogLikelihood(model.likelihood, model).to(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6651bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85533cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150 - Loss: 13.684\n",
      "Epoch 20/150 - Loss: 13.453\n",
      "Epoch 30/150 - Loss: 13.243\n",
      "Epoch 40/150 - Loss: 13.061\n",
      "Epoch 50/150 - Loss: 12.909\n",
      "Epoch 60/150 - Loss: 12.787\n",
      "Epoch 70/150 - Loss: 12.693\n",
      "Epoch 80/150 - Loss: 12.623\n",
      "Epoch 90/150 - Loss: 12.573\n",
      "Epoch 100/150 - Loss: 12.537\n",
      "Epoch 110/150 - Loss: 12.514\n",
      "Epoch 120/150 - Loss: 12.498\n",
      "Epoch 130/150 - Loss: 12.488\n",
      "Epoch 140/150 - Loss: 12.482\n",
      "Epoch 150/150 - Loss: 12.479\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 150\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_X)\n",
    "    loss = -mll(output, model.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344a5d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/botorch-demo/.venv/lib/python3.12/site-packages/botorch/acquisition/analytic.py:331: NumericsWarning: ExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t ExpectedImprovement \t --> \t LogExpectedImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 | Best merit factor so far: 1.3595\n",
      "Iteration 2 | Best merit factor so far: 1.4658\n",
      "Iteration 3 | Best merit factor so far: 1.4658\n",
      "Iteration 4 | Best merit factor so far: 2.9032\n",
      "Iteration 5 | Best merit factor so far: 2.9032\n"
     ]
    }
   ],
   "source": [
    "N_BATCH = 5   # how many BO steps to run\n",
    "\n",
    "for i in range(N_BATCH):\n",
    "    # Re-fit GP on current data\n",
    "    model = SingleTaskGP(train_X=train_X, train_Y=train_Y)\n",
    "    model.likelihood.noise_covar.register_constraint(\"raw_noise\", GreaterThan(1e-5))\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model).to(train_X)\n",
    "    optimizer = Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "    # Train GP hyperparameters\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_X)\n",
    "        loss = -mll(output, model.train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Acquisition function (Expected Improvement)\n",
    "    EI = ExpectedImprovement(model=model, best_f=train_Y.max())\n",
    "\n",
    "    # Sample candidate binary points\n",
    "    candidates = torch.randint(0, 2, (2000, dim), dtype=dtype, device=device)\n",
    "    acq_values = EI(candidates.unsqueeze(1))\n",
    "    new_x = candidates[acq_values.argmax()]\n",
    "\n",
    "    # Evaluate and add to training data\n",
    "    new_y = labs(new_x.unsqueeze(0)).unsqueeze(-1)\n",
    "    train_X = torch.cat([train_X, new_x.unsqueeze(0)], dim=0)\n",
    "    train_Y = torch.cat([train_Y, new_y], dim=0)\n",
    "\n",
    "    print(f\"Iteration {i+1} | Best merit factor so far: {train_Y.max().item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
